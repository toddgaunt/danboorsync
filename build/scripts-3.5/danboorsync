#!/bin/python3
# License: MIT (doc/LICENSE)
# Author: Todd Gaunt

import re
import os
import argparse
import threading
from urllib.parse import urlparse, parse_qs

# package imports
from danboorsync import danbooru
from danboorsync import logger
from danboorsync import util


def main():
    parser = argparse.ArgumentParser(description='Downloads images en masse from Danbooru')

    parser.add_argument('-p', '--pages', metavar='count',
                        type=str, default="1",
                        help='List all page range to download from, \
                                e.g. -p "1-5, 10, 15-20"')

    parser.add_argument('-q', '--quiet', 
                        action="store_true", default=False,
                        help='Turns off output')

    parser.add_argument('-v', '--verbose', 
                        action="count", default=1,
                        help='Turns on more verbose output with levels')

    parser.add_argument('url', metavar='url', 
                    type=str,
                    help='The url to download from')

    parser.add_argument('-o', '--output', metavar='output',
                        type=str,
                        help='The output directory')

    args = vars(parser.parse_args())

    # Makes things verbose
    verbosity = args["verbose"]
    if args["quiet"]:
        args["verbose"] = 0

    # Rename the argument to a shorter variable, keep it unparsed
    # and make sure it's legit
    url = args["url"]
    if urlparse(url).netloc != danbooru.NETLOC:
        logger.error(1, "Invalid url, doesn't match danbooru netloc")


    output = args["output"]
    if not output:
        pu = urlparse(url)
        queries = parse_qs(pu.query)
        if queries == {}:
            output = util.remove_chars(urlparse(url).path)
        elif "tags" in queries.keys():
            output = "_".join(queries["tags"])[0:200]
        else:
            output = "danboorsync"

    if not os.path.exists(output):
        util.make_dir(output)

    # Turn the string of numbers into a usable list
    page_numbers = string_range_parse(args["pages"].split(','))

    # Now download all pages specified
    with util.cd(output):
        for number in page_numbers:
            # Append which page to be downloaded as a query on the url
            page_url = danbooru.appendqueries(url, ["page={}".format(number)])
            print(page_url)

            page_dir = "page{}".format(number)
            if not os.path.exists(page_dir):
                util.make_dir(page_dir)
            # Get md5sums then encode them
            with util.cd(page_dir):
                md5sums = util.get_file_md5sums()

                json = danbooru.jsonize(page_url)
                for post in json:
                    # If the post is missing info, skip it
                    if not danbooru.verify_picture(post):
                        continue
                    if not post["md5"] in md5sums:
                        filename = danbooru.download_post(post)
                        logger.output(1, "{} downloaded in {}".format(\
                                filename, page_dir))
                    else:
                        logger.warning(1, "md5sum match \"{}\" -> \"{}\"".format( \
                                md5sums[post["md5"]], post["md5"]))

def string_range_parse(numbers):
    """Takes a list strings representitave of number ranges, 
    e.g. ['1-3', '32-33', '99'] and converts it to a list of individual numbers.
    The above example would become [1,2,3,32,33,99]"""
    pages = []
    for i in numbers:
        rangematch = re.match(r'\s*(\d+)-(\d+)\s*', i)
        singlematch = re.match(r'\s*(\d+)\s*', i)
        if rangematch != None:
            for j in range(int(rangematch.group(1)), int(rangematch.group(2))+1):
                pages.append(j)
        elif singlematch:
            pages.append(int(singlematch.group(1)))
        else:
            logger.warning(1, "Invalid number range {}".format(i))
    return pages

if __name__ == "__main__":
    main()

# End of file
